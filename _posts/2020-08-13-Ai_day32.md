---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day32) Multinomial Logistic Regression (Softmax Regression)
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

>  [모두를 위한 머신러닝과 딥러닝의 강의(김성훈교수님)](https://hunkim.github.io/ml/)를 참조해서 정리한 수업내용 입니다.

<br>

## Hypothesis

---

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/06.jpeg" alt="png" style="zoom:40%;" />

이 그림처럼 Y값의 범주가 3개 이상인 분류를 `Multinomial Clssification` (다중분류)이라고 한다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/05.jpeg" alt="png" style="zoom:40%;" />

Logistic Regression(Binary Classification)은 좌표상에 표현된 데이터를 2개의 그룹으로 나누는 구분선을 찾고 그 값을 sigmoid함수에 전달하여 Y값을 예측 했다. Multinomial classification에서도 이 개념을 사용한다. <br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/08.jpeg" alt="png" style="zoom:40%;" />

위 그림에서는 A,B,C 세개의 레이블을 가지는 데이터의 분류를 위해 Binary Classification에서 사용한 decision boundary를 여러 개 그려놓은 모습을 확인할 수 있다. 즉, 여러개의 레이블 값으로 분류하기 위해서는 여러개의 Binary Classification이 필요하다.

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/11.jpeg" alt="png" style="zoom:40%;" />

여러개의 Binary Classfication을 구현하기 위해서는 위 그림처럼 여러개의 행렬이 필요하다.

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/12.jpeg" alt="png" style="zoom:40%;" />

이걸 하나의 행렬로 결합한 형태가 위 그림이다. (이 그림에서는 x1,x2,x3 세개의 feature가 입력으로 주어짐)

예측 결과가 A,B,C 중 하나의 값이어야 한다면, 동일한 x features에 대해 A가 될 확률, B가 될 확률, C가 될 확률을 구해야 한다.  

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/17.jpeg" alt="png" style="zoom:40%;" />

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/18.jpeg" alt="png" style="zoom:40%;" />

행렬의 곱셈 값으로 A에 해당하는 2.0, B에 해당하는 1,0, C에 해당하는 0.1의 값을 구했다. 하지만 우리는 앞에서 `sigmoid`를 사용했던 것처럼 0~1사이의 값을 원한다. 위 그림처럼 우리는 각각 A,B,C와 관련된 값들에 대하여 0~1사이의 값이면서 이 값들의 합이 1이 되는 p를 구하고 싶다.

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/19.jpeg" alt="png" style="zoom:40%;" />

이런 값을 구하기 위해 `softmax` 함수를 사용한다. softmax 함수를 거쳐 나온 값은 <b>0~1사이의 값을 가지면서 전체 합이 1이 된다</b>. 즉 각각의 값을 A가 될 확률, B가 될 확률, C가 될 확률로 볼 수 있다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/20.jpeg" alt="png" style="zoom:40%;" />

softmax로 확률값을 구한 후, 그 중에서 하나의 레이블을 선택하기 위해서 `one-hot encoding` 을 사용한다.  `one-hot encoding`은 각각의 확률 값 중 에서 가장 큰 값은 1로 나머지는 0으로 만들어준다.

<br>

<br>

## Cost function

---

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/21.jpeg" alt="png" style="zoom:40%;" />

cost function으로 `CROSS-ENTROPY`를 사용한다. 위 그림에서 `S`는 시스템을 통해 예측한 값이고, `L`은 실제의 값(레이블)이다.

 `S`에 -log를 취한 값과 `L`의 element-wise(요소별 연산) 값을 구한다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/23.jpeg" alt="png" style="zoom:40%;" />

위 그림은 A,B 두가지 레이블만 있다고 가정했다.

실제값 L=[0,1]이다. 첫번째에 해당하는 값이 A, 두번째에 해당하는 값이 B라고 하면 실제 레이블은 `B`이다

그 아래 예측한 값은 [0,1]로 `B`를 예측하고 있다. 이 때 `Cross-Entropy cost function`을 계산하면 0의 값이 나온다.   

결과 값과 반대로 예측한 경우에는 cost값이 무한대로 커지게 된다. 



<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/softmax/28.png" alt="png" style="zoom:40%;" />

<br>

<br>

## 텐서플로우로 구현해보기

---

```python
# softmax_multi_classification
# multi-nomial Classification (다중 분류) : Y값의 범주가 3개 이상인 분류
# 활성화 함수(Activation function) 으로 softmax() 함수가 사용 된다

import tensorflow as tf
import numpy as np
tf.random.set_seed(5)
```


```python
# train data set :
# x_data :  [N,4]  --> [8,4]
x_data = [[1,2,1,1],
          [2,1,3,2],
          [3,1,3,4],
          [4,1,5,5],
          [1,7,5,5],
          [1,2,5,6],
          [1,6,6,6],
          [1,7,7,7]]

# y_data : [N,3] --> [8,3]
y_data = [[0,0,1],  # [2]
          [0,0,1],  # [2]
          [0,0,1],  # [2]
          [0,1,0],  # [1]
          [0,1,0],  # [1]
          [0,1,0],  # [1]
          [1,0,0],  # [0]
          [1,0,0]]  # [0]

x_train = np.array(x_data,dtype=np.float32)
y_train = np.array(y_data,dtype=np.float32)
```


```python
# 변수 초기화 : weight, bias
# (8,4) * (4,3)  = (8,3)
nb_classes = 3 # 분류 갯수
W = tf.Variable(tf.random.normal([4,nb_classes]), name ='weight')
b = tf.Variable(tf.random.normal([nb_classes]), name = 'bias')
```


```python
# hypothesis 예측 함수 : H(X) = softmax(W*X + b)
def logits(X):   #logits -> score
    return tf.matmul(X,W) + b

def hypothesis(X):  #return probabilities
    return  tf.nn.softmax(logits(X))  
```


```python
# 비용 함수 구현 방법 1 : log함수를 사용하여 수식을 직접 표현
# def cost_func():
#     cost = tf.reduce_mean(-tf.reduce_sum(y_train*tf.math.log(hypothesis(x_train)),axis=1))
#     return cost
```


```python
# 비용 함수 구현 방법 2 : tf.nn.softmax_cross_entropy_with_logits() 함수 사용
def cost_func():
    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits(x_train),labels=y_train)
    cost = tf.reduce_mean(cost_i)
    return cost
```


```python
# 경사 하강법
# learning_rate(학습율) 을 0.01로 설정하여 optimizer 객체를 생성
optimizer = tf.keras.optimizers.Adam(lr=0.01)
```


```python
# 학습 시작
print('***** Start Learning!!')
for step in range(5001):
    # cost를 minimize한다
    optimizer.minimize(cost_func,var_list=[W,b])
    
    if step % 1000 == 0:
        print('%04d'%step,'cost: [',cost_func().numpy(),']',
             'W:',W.numpy(),'b:',b.numpy())

print('***** Learning Finished')
```

    ***** Start Learning!!
    0000 cost: [ 5.9294786 ] W: [[-0.17030679 -0.94028634 -0.04964045]
     [-0.7325406   1.3331522  -0.628548  ]
     [ 0.86406636 -0.07899956  2.4388697 ]
     [ 0.77250797  1.2759615   0.9701489 ]] b: [0.2365285  0.8206551  0.73660946]
    1000 cost: [ 0.31467578 ] W: [[-2.2194247e+00  5.2480620e-01  1.1872505e+00]
     [ 1.2104916e-01  2.2530602e-03 -6.3700572e-02]
     [ 2.2870739e+00  1.1651016e+00  6.7435086e-01]
     [ 1.2760866e+00  1.7768894e+00  3.7673813e-01]] b: [-2.71877   -1.5067686  4.317265 ]
    2000 cost: [ 0.16304699 ] W: [[-3.7972443   1.3537493   2.267659  ]
     [ 0.0414911   0.02373775  0.06498498]
     [ 3.5885108   1.0490662  -0.50219786]
     [ 1.0556871   1.817029    0.5562779 ]] b: [-6.1154466 -1.3375344  7.16916  ]
    3000 cost: [ 0.08965951 ] W: [[-5.2343016   2.1662533   3.2086897 ]
     [-0.04391001  0.06965797  0.16019413]
     [ 4.8728337   0.69199216 -1.5755051 ]
     [ 0.73721164  2.0572202   0.7426419 ]] b: [-9.075828  -1.3604162  9.762421 ]
    4000 cost: [ 0.05118698 ] W: [[-6.569132    2.9417915   4.072178  ]
     [-0.12962751  0.1206399   0.24656829]
     [ 6.118547    0.27676854 -2.5946245 ]
     [ 0.3753315   2.3316689   0.953906  ]] b: [-11.740847   -1.3668194  12.113414 ]
    5000 cost: [ 0.029821288 ] W: [[-7.8441830e+00  3.6925721e+00  4.8953791e+00]
     [-2.1366470e-01  1.7167602e-01  3.2927620e-01]
     [ 7.3285356e+00 -1.4589632e-01 -3.5808389e+00]
     [-2.5742422e-03  2.6041079e+00  1.1803545e+00]] b: [-14.22404    -1.3462191  14.310619 ]
    ***** Learning Finished

```python
# 회귀 계수 : weight과 bias출력
print('Weight :',W.numpy())
print('Bias:',b.numpy())
```

    Weight : [[-7.8441830e+00  3.6925721e+00  4.8953791e+00]
     [-2.1366470e-01  1.7167602e-01  3.2927620e-01]
     [ 7.3285356e+00 -1.4589632e-01 -3.5808389e+00]
     [-2.5742422e-03  2.6041079e+00  1.1803545e+00]]
    Bias: [-14.22404    -1.3462191  14.310619 ]

```python
# 예측
# tf.argmax() : 값이 가장 큰 요소의 인덱스 값을 반환
def predict(X):
    return tf.argmax(hypothesis(X),axis=1)

print('***** Predict')

# 학습데이터를 그대로 검증데이터로 사용한 경우
x_test = np.array(x_data,dtype=np.float32)
y_test = np.array(y_data,dtype=np.float32)

preds = predict(x_test)
print(preds.numpy())
print(hypothesis(x_test).numpy())
print(tf.argmax(y_test,1).numpy())
```

    ***** Predict
    [2 2 2 1 1 1 0 0]
    [[6.7201891e-15 4.4788094e-06 9.9999547e-01]
     [3.0885475e-11 6.2573748e-03 9.9374264e-01]
     [8.2859493e-18 3.1583805e-02 9.6841621e-01]
     [5.7242741e-16 9.7510344e-01 2.4896599e-02]
     [5.8690630e-02 9.3963599e-01 1.6734032e-03]
     [3.0668104e-02 9.6914285e-01 1.8900630e-04]
     [9.2271829e-01 7.7280879e-02 9.1235347e-07]
     [9.9905401e-01 9.4603549e-04 1.0147023e-10]]
    [2 2 2 1 1 1 0 0]



