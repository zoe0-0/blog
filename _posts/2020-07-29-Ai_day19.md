---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day19) 선형회귀(Linear Regression))
tags:
  - 머신러닝
---

<br>

### 선형회귀(Linear Regession) : 1차함수, 직선의 방정식

- weight(가중치) : 입력변수가 출력에 미치는 정도를 설정
- bias(편향) : 기본 출력 값이 활성화 되는 정도를 설정 
- 비용함수(cost function) : 2차함수, (예측값-실제값)^2
  - cost(비용) = 오차 = 에러 = 손실(loss)
  - cost(w,b) = (H(x) - y)^2 
- 예측(가설,Hypothesis)함수: H(x) : w*x + b
- 경사하강법(Gradient Descent Algoirthm) : 비용(cost)이 가장 작은 기울기 값을 구하는 알고리즘

<br>

### 비용함수의 구현


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```python
def cost(x,y,w):
    c=0
    for k in range(len(x)):
        hx = w*x[k]
        loss = (hx-y[k])**2
        c += loss
    return c/len(x)
    
x = [1,2,3] #입력값, 독립변수
y = [1,2,3] #답, 실제값(결정값), 종속변수

print(cost(x,y,-1))
print(cost(x,y,0))
print(cost(x,y,1)) # => 최적의 weight
print(cost(x,y,2))
```

    18.666666666666668
    4.666666666666667
    0.0
    4.666666666666667

<br>

```python
# 비용함수의 시각화 : x축을 weight, y축을 cost로 하는 2차함수
for k in range(-30,50):
    w=k/10
    c = cost(x,y,w)
    plt.plot(w,c,'o')
    
plt.title('cost function') 
plt.xlabel('weight');
plt.ylabel('cost')
plt.show()
```

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/basic_ml_files/basic_ml_5_0.png)

<br>

### 경사하강법(Gradient Descent Algoirthm)

- 미분 : f(x)= x^n => f'(x)=n*x^(n-1)


```python
def gradient_descent(x,y,w):
    c = 0;
    for k in range(len(x)):
        hx = w*x[k]
        loss = (hx-y[k])*x[k] #비용함수의 미분
        c += loss
    return c/len(x)
        
        #비용함수의 미분
        #cost(w) = (w*x[k] - y[k])^2의 미분
        #cost(w) = w^2*x[k]^2 - 2*w*x[k]*y[k] - y[k]^2
        #cost'(w) = 2w*x[k]^2 - 2*x[k]*y[k] = 2*x[k](w*x[k] - y[k]) = 2*x[k](hx - y[k])
        #x[k](hx - y[k]) 곱하기2 생략        
```


```python
#학습시작
print('------------start learning')
w,old = 10,100
for k in range(1000):
    c = cost(x,y,w)
    grad = gradient_descent(x,y,w)
    #가중치 업데이트
    w -= 0.1*grad #0.1=학습율(learning rate)
    #print('[%03d]'%k,'cost:',c,old,w)
    if c>=old and abs(c-old) <1.0e-15: #cost의 변화가 없을 때
        break
    old = c        
print('------------end learning')
print('weight:',w,'train:',k+1)
```

    ------------start learning
    ------------end learning
    weight: 1.0 train: 64

<br>

### Linear Regression


```python
#비용함수 구현
def cost(x,y,w):
    c=0
    for k in range(len(x)):
        hx = w*x[k]
        loss = (hx-y[k])**2
        c += loss
    return c/len(x)


#경사하강법 구현
def gradient_descent(x,y,w):
    c = 0;
    for k in range(len(x)):
        hx = w*x[k]
        loss = (hx-y[k])*x[k] #비용함수의 미분
        c += loss
    return c/len(x)


#학습(fit)함수 구현
def fit(x,y):
    print('------------start learning')
    w,old = 10,100
    for k in range(1000):
        c = cost(x,y,w)
        grad = gradient_descent(x,y,w)
        #가중치 업데이트
        w -= 0.1*grad #0.1=학습율(learning rate)
        #print('[%03d]'%k,'cost:',c,old,w)
        if c>=old and abs(c-old) <1.0e-15: #cost의 변화가 없을 때
            break
        old = c        
    print('------------end learning')
    return w
   
    
#예측(predict) 함수
def predict(x,w):
    hx = w*np.array(x)
    return list(hx)
```

<br>


```python
x_train = [1,2,3,4,5]
y_train = [2,4,6,8,10]
w = fit(x_train,y_train)
predict([6,7,8],w)
```

    ------------start learning
    ------------end learning
    
    [12.0, 14.0, 16.0]

