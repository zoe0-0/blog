---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day31) Logistic Regression 
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

## [Logistic Regression](https://ko.wikipedia.org/wiki/로지스틱_회귀)

> 로지스틱 회귀의 목적은 일반적인 [회귀 분석](https://ko.wikipedia.org/wiki/회귀_분석)의 목표와 동일하게 [종속 변수](https://ko.wikipedia.org/wiki/독립_변수와_종속_변수)와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. 이는 독립 변수의 선형 결합으로 종속 변수를 설명한다는 관점에서는 [선형 회귀](https://ko.wikipedia.org/wiki/선형_회귀) 분석과 유사하다. 하지만 로지스틱 회귀는 [선형 회귀](https://ko.wikipedia.org/wiki/선형_회귀) 분석과는 다르게 종속 변수가 범주형 데이터를 대상으로 하며 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 ([classification](https://en.wikipedia.org/wiki/classification)) 기법으로도 볼 수 있다.

<br>

### Logistic Regression : 2진 분류(Binary Classification)

- 2진 분류의 활성화 함수로는 `sigmoid`가 사용됨

- 시그모이드 함수를 통해 얻어낸 Hypothesis는 X 입력값들에 따른 Y값을 [0,1]로 압축한 것이다. 
- Y값은 사건이 발생할 확률을 의미한다. 0 ~ 1 사이의 값을 가지는 임계값을 결정해서 사건이 발생할 확률(Y)이 어느 수치 이상이여야 1로 분류할지 정한다

```python
import math
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1./(1.+math.e**-z)
 
print(sigmoid(0))
```

    0.5

```python
#시각화
x,y = [],[]
for k in range(-100,101):
    n = sigmoid(k/10)
    x.append(k/10)
    y.append(n)
    
plt.plot(x,y,'r')
plt.grid()
plt.show()
```

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/sigmoid_files/sigmoid.png)

<br>

<br>

### Logistic Regression의 cost 함수 

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/sigmoid_files/costfunction.png" alt="png" style="zoom:80%;" />

- 실제값과 예측값이 비슷하거나 같으면 cost(비용)값은 작아지고, 실제값과 예측값의 차이가 커질수록 cost(비용)값이 커진다

- 실제 y값이 1일 때 ) C(H(x),y) = -log(H(x)) 

  ​							  H(x)=1 이면, cost = -log(1) = 0 (예측 성공)

  ​                              H(x)=0 이면, cost = -log(0) = 무한대로 (예측 실패)

- 실제 y값이 0일 때 ) C(H(x),y) = -log(1-H(x)) 

  ​							  H(x)=0 이면, cost = -log(1-0) = 0 (예측 성공)

  ​                              H(x)=1 이면, cost = -log(0) = 무한대로  (예측 실패)

- 즉, 예측이 틀릴경우 cost가 커지게 된다
- `C(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x))` : 두개의 비용함수를 합성

<br>

### 텐서플로우로 구현해보기


```python
import tensorflow as tf
import numpy as np
tf.random.set_seed(5)
```


```python
# x_data : [6,2]
x_data = [[1,2],
          [2,3],
          [3,1],
          [4,3],
          [5,3],
          [6,2]]

# y_data : [6,1]
y_data = [[0],
          [0],
          [0],
          [1],
          [1],
          [1]]

x_train = np.array(x_data,dtype=np.float32)
y_train = np.array(y_data,dtype=np.float32)
```


```python
#변수초기화
#(6,2)*(2,1) => (6*1)
W = tf.Variable(tf.random.normal([2,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')
```


```python
#hypothesis : 예측함수, H(x) = sigmoid(X*W+b)
def hypothesis(X):
    return sigmoid(tf.matmul(X,W)+b) #0과1사이의 값을 출력
```


```python
#비용함수,logloss
#C(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x))
def cost_func():
    #cost = tf.reduce_mean(-y_train*tf.math.log(hypothesis(x_train))-(1-y_train)*tf.math.log(1-hypothesis(x_train)))
    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + (1-y_train)*tf.math.log(1-hypothesis(x_train)))   #-를 앞으로 빼냄
    return cost
```


```python
#학습
#Minimize Cost - Gradient Descent Alogorithm 
#비용함수의 가장 작은 값을 구하도록 미분해서 기울기를 구하고 경사타고 내려가기
optimizer = tf.keras.optimizers.Adam(lr=0.01)
print("start learnig")
for step in range(10001):
    #cost를 minimize
    optimizer.minimize(cost_func,var_list = [W,b])
    if step%1000 == 0:
        print('%04d'%step,'cost: ',cost_func().numpy(),'W:',W.numpy(),'b:',b.numpy())
        print()
print("learning finish")
```

    start learnig
    0000 cost:  1.661137 W: [[-0.17030667]
     [-0.9402863 ]] b: [0.23652855]
    
    1000 cost:  0.17934637 W: [[1.3178085 ]
     [0.19723557]] b: [-4.7614303]
    
    2000 cost:  0.07253331 W: [[2.0121026]
     [0.8405848]] b: [-8.711954]
    
    3000 cost:  0.037006143 W: [[2.6189137]
     [1.2299302]] b: [-11.698347]
    
    4000 cost:  0.020763235 W: [[3.1709244]
     [1.5396552]] b: [-14.285828]
    
    5000 cost:  0.012184973 W: [[3.6927986]
     [1.8153361]] b: [-16.680555]
    
    6000 cost:  0.0073189805 W: [[4.197647 ]
     [2.0743663]] b: [-18.973686]
    
    7000 cost:  0.0044522597 W: [[4.6926694]
     [2.3246486]] b: [-21.210659]
    
    8000 cost:  0.0027277553 W: [[5.1819415]
     [2.5701625]] b: [-23.415737]
    
    9000 cost:  0.0016780436 W: [[5.667812 ]
     [2.8129678]] b: [-25.602293]
    
    10000 cost:  0.001034755 W: [[6.151635 ]
     [3.0542402]] b: [-27.777964]
    
    learning finish

<br>

### 정확도 측정

- `tf.equal(x, y)` : x, y를 비교하여 boolean 값을 반환
- `tf.cast(x,dtype=tf.float32)` : x를 특정 타입으로 변환

```python
def predict(X):
    return tf.cast(hypothesis(X)>0.5,dtype=tf.float32)  #예측값이 0.5보다 크면 1.0으로 작으면 0.0으로 변환

preds = predict(x_train)
# tf.cast(tf.equal(preds,y_train),dtype=tf.float32) #예측값과 실제 결과가 맞으면 true => 1.0 로 변환, 틀리면 false => 0.0  
accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y_train),dtype=tf.float32))   #예측이 모두 맞을 경우 정확도 1.0
print('accuracy',accuracy.numpy())
```

    accuracy 1.0

<br>

### Classification Threshold (임계값)

- `tf.cast(hypothesis(X)>0.5,dtype=tf.float32)` 여기에서 0.5가 임계값
- 대부분 알고리즘에서 기본 임계값은 0.5이다
- 하지만, 필요에 따라 임계값을 변경할 수 있다. 
- 예를 들어 암을 진단하는 로지스틱회귀 모델의 경우, 임계값을 낮추면 더 안전하게(?) 암환자를 진단할 수 있다.
  - 암 진단의 경우 암이 아닌데 암이라고 진단하는 것은 큰 문제가 아니지만
  - 암인데 암이 아니라고 진단하는 것은 매우 위험하다. 
  - 따라서 임계값을 조금 낮추면 더 낮은 분류 확률로도 암이라고 예측될 수 있기 때문에 오분류가 많아지더라도 비교적 안전

