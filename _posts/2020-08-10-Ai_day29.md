---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day29) 텐서플로우로 Linear Regression 구현하기
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

> 학습을 한다는 건 (W,b)를 조절해서 cost(W,b)값을 minimize 하는 것!

```python
H(x) = Wx+b # Hypothesis(가설) : 데이터에 잘 맞는 적절한 선 
cost(W,b) = (H(x)-y)^2의 평균 
```

<br>

### 학습 데이터


```python
import tensorflow as tf
import numpy as np
tf.random.set_seed(5)

x_train = [1.0, 2.0, 3.0, 4.0, 5.0]
y_train = [1.1, 2.2, 3.3, 4.4, 5.5]
```

<br>

###  텐서플로우의 Variable => 텐서플로우가 학습하는 과정에서 자체적으로 변경시키는 값. Trainable Variable


```python
W = tf.Variable(tf.random.normal([1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')
```

<br>

### 예측함수와 비용함수


```python
#hypothesis : 예측함수, H(x) = W*x + b
def hypothesis(X):
    return X*W + b
```


```python
#비용함수, (H(x)-y)^2의 평균
def cost_func():
    cost = tf.reduce_mean(tf.square(hypothesis(x_train)-y_train))
    return cost
```

<br>

### Gradient Descent -> Minimize


```python
#경사하강법
#학습율을 0.01로 설정하여 optimizer 객체를 생성
#optimizer = tf.compat.v1.train.GradientDescentOptimizer(learningrate=0.1)
optimizer = tf.keras.optimizers.Adam(lr=0.01)
```


```python
#학습시작
print("start learnig")
for step in range(3001):
    #cost를 minimize
    optimizer.minimize(cost_func,var_list = [W,b])
    if step%100 == 0:
        print('%04d'%step,'cost: ',cost_func().numpy(),'W:',W.numpy(),'b:',b.numpy())
print("learning finish")
```

    start learnig
    0000 cost:  5.684342e-14 W: [1.0999998] b: [7.1708365e-07]
    0100 cost:  5.684342e-14 W: [1.0999998] b: [7.17178e-07]
    0200 cost:  5.684342e-14 W: [1.0999998] b: [7.172771e-07]
    0300 cost:  5.684342e-14 W: [1.0999998] b: [7.173812e-07]
    0400 cost:  5.684342e-14 W: [1.0999998] b: [7.17491e-07]
    0500 cost:  5.684342e-14 W: [1.0999998] b: [7.1760604e-07]
    0600 cost:  5.684342e-14 W: [1.0999998] b: [7.177271e-07]
    0700 cost:  5.684342e-14 W: [1.0999998] b: [7.178549e-07]
    0800 cost:  5.684342e-14 W: [1.0999998] b: [7.179889e-07]
    0900 cost:  5.684342e-14 W: [1.0999998] b: [7.1812934e-07]
    1000 cost:  1.4210855e-14 W: [1.1] b: [-9.724891e-08]
    1100 cost:  1.4210855e-14 W: [1.1] b: [-1.1810804e-07]
    1200 cost:  1.4210855e-14 W: [1.1] b: [-1.11069184e-07]
    1300 cost:  1.4210855e-14 W: [1.1] b: [-1.12174995e-07]
    1400 cost:  1.4210855e-14 W: [1.1] b: [-1.1593837e-07]
    1500 cost:  1.4210855e-14 W: [1.1] b: [-1.15472005e-07]
    1600 cost:  1.4210855e-14 W: [1.1] b: [-1.18466666e-07]
    1700 cost:  1.4210855e-14 W: [1.1] b: [-1.1714731e-07]
    1800 cost:  1.4210855e-14 W: [1.1] b: [-1.0869133e-07]
    1900 cost:  1.4210855e-14 W: [1.1] b: [-1.099738e-07]
    2000 cost:  1.4210855e-14 W: [1.1] b: [-1.15677395e-07]
    2100 cost:  1.4210855e-14 W: [1.1] b: [-1.1521077e-07]
    2200 cost:  1.4210855e-14 W: [1.1] b: [-1.2100122e-07]
    2300 cost:  1.4210855e-14 W: [1.1] b: [-1.1884766e-07]
    2400 cost:  1.4210855e-14 W: [1.1] b: [-1.0645339e-07]
    2500 cost:  1.4210855e-14 W: [1.1] b: [-1.0484989e-07]
    2600 cost:  1.4210855e-14 W: [1.1] b: [-1.2213258e-07]
    2700 cost:  1.4210855e-14 W: [1.1] b: [-1.1720309e-07]
    2800 cost:  1.4210855e-14 W: [1.1] b: [-1.03889406e-07]
    2900 cost:  1.4210855e-14 W: [1.1] b: [-1.0708044e-07]
    3000 cost:  1.4210855e-14 W: [1.1] b: [-1.1481613e-07]
    learning finish

<br>

```python
#회귀계수 : Weight과 bias 출력
print('Weight:',W.numpy())
print('bias:',b.numpy())
```

    Weight: [1.1]
    bias: [-1.1481613e-07]

<br>

```python
#예측함수
print("predict")
print('x=6.0, H(x)=',hypothesis(6.0).numpy())
print('x=9.5, H(x)=',hypothesis(9.5).numpy())
```

    predict
    x=6.0, H(x)= [6.6000004]
    x=9.5, H(x)= [10.45]

