---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day33,34) XOR문제, Neural Net으로 해결하기
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

>  [모두를 위한 머신러닝과 딥러닝의 강의(김성훈교수님)](https://hunkim.github.io/ml/)를 참조해서 정리한 수업내용 입니다.

<br>

### [퍼셉트론](https://zoe0-0.github.io/blog/Ai_day18/)

> 위 포스팅에서도 확인할 수 있듯이, 단층 퍼셉트론으로는 XOR 문제를 해결할 수 없었다. 하나의 퍼셉트론은 입력값 (X)와 가중치(W)를 곱해서 하나의 결정경계를 만드는데, XOR문제는 하나의 결정경계로는 해결할 수 없기 때문이다. 하지만 3개의 퍼셉트론을 사용하면 XOR문제를 해결할 수 있다.

<br>

## XOR using NN

---

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/1.png" alt="png" style="zoom:40%;" />

3개의 퍼셉트론을 통해 XOR 문제를 풀 수 있었다. 이것이 하나의 Neural Network이다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/2.png" alt="png" style="zoom:40%;" />

왼쪽 두개의 퍼셉트론에서 각각의 W와 b를 Multinomial Classfication 처럼 하나의 Vector로 만들 수 있다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/3.png" alt="png" style="zoom:40%;" />

수식으로 표현하면 위 그림과 같다

<br>

## 텐서플로우로 구현해보기

---

```python
#단층 신경망으로 해결할 수 없던 XOR문제를 다층 신경망으로 해결함

import tensorflow as tf
import numpy as np
tf.random.set_seed(5)

# train data set 
x_data = [[0,0],
          [0,1],
          [1,0],
          [1,1]]

y_data = [[0],
          [1],
          [1],
          [0]]

x_train = np.array(x_data,dtype=np.float32)
y_train = np.array(y_data,dtype=np.float32)
```


```python
# Layer1
W1 = tf.Variable(tf.random.normal([2,2]),name = 'weight')
b1 = tf.Variable(tf.random.normal([2]), name = 'bias')

def layer1(X):
    return tf.sigmoid(tf.matmul(X,W1) + b1 )
```


```python
# (4,2) * (2,1) = (4,1)
W2 = tf.Variable(tf.random.normal([2,1]),name = 'weight')
b2 = tf.Variable(tf.random.normal([1]), name = 'bias')

# hypothesis 예측 함수(방정식)  , H(x) = sigmoid(W * X + b)
def hypothesis(X):
    return  tf.sigmoid(tf.matmul(layer1(X),W2) + b2 )
```


```python
# 비용함수 : logloss
def cost_func():
    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + (1-y_train)*
                       tf.math.log(1-hypothesis(x_train)))
    return cost
```


```python
# 경사 하강법
# learning rate (학습율) 을 0.01로 설정하여  optimizer객체를 생성
# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
optimizer = tf.keras.optimizers.Adam(lr=0.01)
```


```python
# 학습 시작
print('***** Start Learning!!')
for step in range(5001):
    # cost를 minimize한다
    optimizer.minimize(cost_func, var_list=[W1,b1,W2,b2])
    
    if step % 1000 == 0:
        print('%04d'%step, 'cost: [', cost_func().numpy(), ']  W1: ', W1.numpy(), '  b1: ', b1.numpy(),'W2: ', W2.numpy(), '  b2: ', b2.numpy())
        
print('***** Learning Finished!!')
```

    ***** Start Learning!!
    0000 cost: [ 0.7040971 ]  W1:  [[-0.19030488 -0.9402918 ]
     [-0.04963873 -0.73254627]]   b1:  [0.21652977 0.8206513 ] W2:  [[ 1.4565471]
     [-1.0348129]]   b2:  [0.02700325]
    1000 cost: [ 0.02550175 ]  W1:  [[-5.024761  -6.6669364]
     [-4.994727  -6.5032763]]   b1:  [7.5096507 2.915475 ] W2:  [[ 8.87536 ]
     [-8.637342]]   b2:  [-4.2820168]
    2000 cost: [ 0.0068961554 ]  W1:  [[-5.944842 -7.574677]
     [-5.917539 -7.41493 ]]   b1:  [8.922688 3.408215] W2:  [[ 11.296161]
     [-10.965301]]   b2:  [-5.5542603]
    3000 cost: [ 0.0029871406 ]  W1:  [[-6.4519587 -8.081456 ]
     [-6.426093  -7.9230824]]   b1:  [9.694395 3.674915] W2:  [[ 12.855274]
     [-12.503557]]   b2:  [-6.3560357]
    4000 cost: [ 0.0015255288 ]  W1:  [[-6.8228345 -8.453764 ]
     [-6.797982  -8.296077 ]]   b1:  [10.256575   3.8682897] W2:  [[ 14.117276]
     [-13.758825]]   b2:  [-6.999548]
    5000 cost: [ 0.00084185496 ]  W1:  [[-7.127812  -8.760641 ]
     [-7.1037564 -8.603351 ]]   b1:  [10.717856   4.0264554] W2:  [[ 15.241033]
     [-14.8807  ]]   b2:  [-7.569965]
    ***** Learning Finished!!

```python
# predict 
# accuracy computation (정확도 측정)
def predict(X):
    return tf.cast(hypothesis(X) > 0.5, dtype = tf.float32)

accuracy = tf.reduce_mean(tf.cast(tf.equal(predict(x_train),y_train),
                                     dtype = tf.float32))

print("Hypothesis:\n",hypothesis(x_train).numpy(), 
      "\nPredict:\n",predict(x_train).numpy(),
      "\nAccuracy:",accuracy.numpy())   # Accuracy: 0.5
```

    Hypothesis:
     [[9.5849548e-04]
     [9.9919170e-01]
     [9.9920160e-01]
     [8.0075976e-04]] 
    Predict:
     [[0.]
     [1.]
     [1.]
     [0.]] 
    Accuracy: 1.0











