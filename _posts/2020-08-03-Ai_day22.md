---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day22) 분류(Classfication)와 결정트리(Decision Tree) 
tags:
  - 머신러닝
---

<br>

## 분류(Classification) 

> 학습 데이터로 주어진 데이터의 피쳐와 레이블값(결정값, 클래스값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을  때 미지의 레이블 값을 예측

<br>

### sklearn의 분류 모델 클래스

- Decision Tree
- Logistic Regression
- Naive Bayes(나이브 베이즈)
- Support Vector Machine
- Nearest Neighbor(최소근접 알고리즘)

<br>

## 결정트리 (Decision Tree)

- 대표적인 분류 학습 모델, 회귀도 가능, 스무고개와 유사
- 루트 노드 --> 규칙 노드(중간노드) --> 리프노드(끝노드)
- 장점 : 쉽고 직관적이다. 피쳐 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않다.
- 단점 : 과적합으로 알고리즘 성능이 떨어진다. 따라서 트리의 크기를 사전에 제한하는 튜닝이 필요

<br>

### 용어 정리

- <b>불순도(Impurity)</b>

  해당범주안에 서로 다른 데이터가 얼마나 섞여 있는지를 말한다, 불확실성, 무질서도<br>

- <b>데이터의 균일도</b>

  다양성이 낮을수록 데이터의 균일도는 높다. <br>
  예를 들어 검은공과 하얀공이 적절히 섞인 A, 약간의 하얀공과 대부분의 검은공으로 이루어져 있는 B, 모두 검은공으로 이루어져 있는 C가 있다고 할 때, 
  C > B > A 순으로 균일도가 높다고 할 수 있다. 

- <b>균일도 기반 규칙조건</b>

  정보 균일도가 높은 데이터세트를 먼저 선택하도록 규칙조건을 만들어야 한다.

- <b>정보 균일도 측정방법</b>

  대표적인 방법으로는 엔트로피를 이용한 정보이득 지수와 지니 계수가 있다.

- <b>엔트로피</b>

  주어진 데이터 집합의 혼잡도를 의미한다. 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮다.

- <b>정보이득(Information Gain)</b>

  정보이득 = 1 - 엔트로피지수
  즉, 데이터의 혼잡도가 작을수록(엔트로피가 낮을수록) 정보이득 지수는 커진다. 결정트리는 정보이득이 높은 속성을 기준으로 분할한다.

- <b>지니계수</b>

  1 - (각 범주별 데이터 비율의 제곱의 합), 0 이면 최소(끝노드) , 1이면 최대¶<br>
  지니 불순도는 집합에 이질적인 것이 얼마나 섞였는지를 측정하는 지표이다. 어떤 집합에서 한 항목을 뽑아 무작위로 라벨을 추정할 때 틀릴 확률을 말한다. 집합에 있는 항목이 모두 같다면 지니 불순도는 최솟값(0)을 갖게 되며 이 집합은 완전히 순수하다고 할 수 있다.

<br>

###  Iris(붓꽃) 품종 예측하기


```python
from sklearn.datasets import load_iris 
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```


```python
iris = load_iris()
iris #sklearn.utils.Bunch

#x값, 피쳐만 추출
iris_data = iris.data
#print(iris_data) #(150, 4) : 2차원 ndarray
print(iris.feature_names) #sepal:꽃받침, petal:꽃잎
```

    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

<br>

```python
#y값, 답(label)만 추출
iris_label = iris.target
print(iris_label)
print(iris.target_names)  #iris의 품종 : [0:'setosa', 1:'versicolor', 2:'virginica']
```

    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
     2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
     2 2]
    ['setosa' 'versicolor' 'virginica']

<br>

```python
iris_df = pd.DataFrame(data = iris_data, columns=iris.feature_names)
iris_df['label'] = iris_label
print(iris_df['label'].value_counts())
iris_df.head()
```

    2    50
    1    50
    0    50
    Name: label, dtype: int64





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

</div>

<br>

### train data set과 test데이터로 분리하기

- scikit-learn.model_selection 패키지 안에 train_test_split 모듈을 활용하여 손쉽게 train set과 test set을 분리할 수 있다.
- 옵션값
  - `test_size` : 테스트 셋 구성의 비율을 나타냅니다. 0.2는 전체 데이터 셋의 20%를 테스트 셋으로 지정하겠다는 의미. default 값은 0.25
  - `shuffle` : default=True. split을 해주기 이전에 섞을건지 
  - `stratify` : default=None 입니다. stratify 값을 label로 지정해주면 각각의 class 비율을 train data set과 test data set에 유지해 줍니다. (어느 한쪽에 쏠려서 분배되는 것을 방지하기 위해)
  - `random_state` : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)


```python
X_train,X_test,y_train,y_test = train_test_split(iris_data,iris_label,test_size=0.2,random_state=11, stratify = iris_label)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (120, 4) (30, 4) (120,) (30,)

<br>


```python
dt_clf = DecisionTreeClassifier(random_state=11)
dt_clf.fit(X_train,y_train) #DecisionTreeClassifier 학습
```


    DecisionTreeClassifier(random_state=11)

<br>

### 결정트리의 주요 하이퍼 파라미터

- `max_depth` : 트리의 최대 깊이를 규정
- `max_features` : 최적의 분할을 위해 고려해야 할 최대 피쳐 갯수
- `min_sample_split` : 샘플이 최소한 몇 개 이상이어야 split(하위노드로 분리), 과적합 제어하기 위해 사용 (default=2)
- `min_sample_leaf` : 말단노드가 되기위한 최소한의 샘플 데이터 수, 노드가 되려면 가지고 있어야할 최소 샘플 수
- `max_leaf_nodes` : 말단 노드의 최대 갯수

```python
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=11, splitter='best')
```

<br>


```python
pred = dt_clf.predict(X_test)
pred,y_test
```


    (array([2, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,
            0, 1, 0, 0, 2, 1, 0, 1]),
     array([2, 2, 2, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,
            0, 1, 0, 0, 2, 1, 0, 1]))

<br>


```python
#정확도 측정 : accuracy
from sklearn.metrics import accuracy_score,classification_report
print('정확도:{:.4f}'.format(accuracy_score(y_test,pred)))
```

    정확도:0.9333

<br>

### 결정트리 모델의 시각화


```python
from sklearn.tree import export_graphviz
export_graphviz(dt_clf,out_file='tree.dot', class_names=iris.target_names, feature_names = iris.feature_names, impurity=True, filled=True)
```

```shell
### graphviz 설치방법
brew install graphviz
pip install pyparsing
pip install graphviz
pip install pydot
conda install graphviz
```

<br>


```python
import graphviz

f = open('tree.dot')
dot_graph = f.read()
graphviz.Source(dot_graph)
```

<br>

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/graphviz_files/tree.svg)

