---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day25) 2진분류 성능 평가 지표
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

## 분류(2진 분류) 성능 평가 지표 

* 정확도(Accuracy) - 불균형한 레이블 값 분포에서 모델성능 판단할 경우 적합하지 않다
* 오차행렬(confusion Matrix)
* 정밀도(Precison)
* 재현율(Recall)
* F1 스코어
* ROC AUC

<br>

## 오차 행렬(confusion Matrix)

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/confusion_matrix.png?raw=true" alt="png" style="zoom:67%;" />

- <b>정확도(Accuracy)</b> = 맞은갯수/전체갯수 = `(TN + TP)/( TN + FP + FN + TP) `

- <b>정밀도(Precision)</b> = `TP/(FP + TP)`, 예측을  Positive로 한 것 중 Positive답을 맞춘 비율, 양성 예측도
  - `TP/(FP + TP)` 이니까 FP(Negative(음성)인데 Positive(양성)이라고 잘못판단한 경우) 값이 적어야 정밀도가 올라간다
  - 정밀도가 중요 지표인 경우 => Negative(음성) 데이터를 Positive(양성) 데이터로 잘못 판단하게 되면 큰 영향이 발생하는 경우   
  - ex) 스팸메일 분류할 때 음성(스팸X)인데 양성(스팸메일)이라고 잘못판단하면 제대로 메일을 받지 못함

- <b>재현율(Recall)</b> = `TP/(FN + TP)`, 실제 값이 Positive인 것 중 Positive답을 맞춘 비율, 민감도, TPR(True Positive Rate)
  - `TP/(FN + TP)` 이니까, FN(Positive(양성)인데 Negative(음성)이라고 잘못판단한 경우)값이 적어야 재현율이 올라간다. 
  - 재현율이 중요 지표인 경우 => Positive(양성) 데이터를 Negative(음성) 데이터로 잘못 판단하게 되면 큰 영향이 발생하는 경우 
  - ex) 암진단, 금융사기 판별
- 정밀도와 재현율의 Trade-off : 정밀도와 재현율은 상호 보완적인 수치여서 어느 한쪽을 강제로 높이면 다른 하나의  수치는 낮아진다.

<br>

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
```

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
def get_clf_eval(y_test,pred):
    confusion = confusion_matrix(y_test,pred)
    accuracy = accuracy_score(y_test,pred)
    precision = precision_score(y_test,pred)
    recall = recall_score(y_test,pred)
    print('오차행렬')
    print(confusion)
    print('정확도:{:.4f}, 정밀도:{:.4f}, 재현율:{:.4f}'.format(accuracy,precision,recall))
```


```python
titanic_df = pd.read_csv('titanic_train.csv')
#titanic_df.head()
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived',axis=1)
X_train,X_test,y_train,y_test = train_test_split(X_titanic_df,y_titanic_df,test_size=0.2,random_state=11)

lr_clf = LogisticRegression()
lr_clf.fit(X_train,y_train)
pred = lr_clf.predict(X_test)
get_clf_eval(y_test,pred)
```

    오차행렬
    [[104  14]
     [ 13  48]]
    정확도:0.8492, 정밀도:0.7742, 재현율:0.7869

<br>

### Precision(정밀도) / Recall(재현율)  Trade-off

`predict_proba( )` : 개별 클래스의 예측 확률을 m*n(m:입력값의 레코드 수, n:클래스 값 유형) 형태로 반환. 각 열은 개별 클래스의 예측 확률이다. 이진 분류에서 첫번째 컬럼은 0 Negative의 확률, 두번째 컬럼은 1 Positive의 확률


```python
pred_prob = lr_clf.predict_proba(X_test)
print("predict_prob 결과")  #[0이될 확률, 1이될확률]
print(pred_prob.shape)
print(pred_prob[:3])
print("-"*50)

print("predict 결과")
pred = lr_clf.predict(X_test)
print(pred.shape)
print(pred[:3])
print("-"*50)

pred_prob_result = np.concatenate([pred_prob,pred.reshape(-1,1)],axis=1)
pred_prob_result
```

    predict_prob 결과
    (179, 2)
    [[0.46189204 0.53810796]
     [0.87872347 0.12127653]
     [0.87720675 0.12279325]]
    --------------------------------------------------
    predict 결과
    (179,)
    [1 0 0]
    --------------------------------------------------
    
    array([[0.46189204, 0.53810796, 1.        ],
           [0.87872347, 0.12127653, 0.        ],
           [0.87720675, 0.12279325, 0.        ],
           [0.88262339, 0.11737661, 0.        ],
           [0.85523322, 0.14476678, 0.        ],
           [0.88224197, 0.11775803, 0.        ],
           [0.88846262, 0.11153738, 0.        ],
           [0.20879543, 0.79120457, 1.        ],
           [0.78286889, 0.21713111, 0.        ],
           [0.36935649, 0.63064351, 1.        ],
       ......

<br>

### 사이킷런의 `Binarizer` 클래스 : 정해진 threshold 보다 같거나 작으면 0, 크면 1 반환


```python
from sklearn.preprocessing import Binarizer
X = [[1,-1,2],
     [2,0,0],
     [0,1.1,1.2]]

#threshold 기준값보다 같거나 작으면 0, 크면 1을 반환
binarizer = Binarizer(threshold=1.1)
print(binarizer.fit_transform(X))
```

    [[0. 0. 1.]
     [1. 0. 0.]
     [0. 0. 1.]]

<br>

```python
from sklearn.preprocessing import Binarizer
custom_threshold=0.5
pred_prob1 = pred_prob[:,1].reshape(-1,1)  #positive클래스 컬럼 하나만 추출해서 Binarizer 적용
binarizer = Binarizer(threshold=custom_threshold).fit(pred_prob1)
custom_predict = binarizer.transform(pred_prob1)
get_clf_eval(y_test,custom_predict)
```

    오차행렬
    [[104  14]
     [ 13  48]]
    정확도:0.8492, 정밀도:0.7742, 재현율:0.7869

<br>

### 임계값이 낮아질수록 재현율은 올라가고 정밀도는 내려간다


```python
thresholds = [0.4, 0.45, 0.50, 0.55, 0.60]

def get_eval_by_threshold(y_test,pred_prob1,thresholds):
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_prob1)
        custom_predict = binarizer.transform(pred_prob1)
        print('임계값:',custom_threshold)
        get_clf_eval(y_test,custom_predict)
        print("-"*50)
        
get_eval_by_threshold(y_test,pred_prob1,thresholds)
```

    임계값: 0.4
    오차행렬
    [[98 20]
     [10 51]]
    정확도:0.8324, 정밀도:0.7183, 재현율:0.8361
    --------------------------------------------------
    임계값: 0.45
    오차행렬
    [[103  15]
     [ 12  49]]
    정확도:0.8492, 정밀도:0.7656, 재현율:0.8033
    --------------------------------------------------
    임계값: 0.5
    오차행렬
    [[104  14]
     [ 13  48]]
    정확도:0.8492, 정밀도:0.7742, 재현율:0.7869
    --------------------------------------------------
    임계값: 0.55
    오차행렬
    [[109   9]
     [ 15  46]]
    정확도:0.8659, 정밀도:0.8364, 재현율:0.7541
    --------------------------------------------------
    임계값: 0.6
    오차행렬
    [[112   6]
     [ 16  45]]
    정확도:0.8771, 정밀도:0.8824, 재현율:0.7377
    --------------------------------------------------

<br>

### `precision_recall_curve( )` : 임계값에 따른 정밀도-재현율 값 추출


```python
from sklearn.metrics import precision_recall_curve

# 레이블 값이 1일때의 예측 확률을 추출 
pred_proba_class1 = lr_clf.predict_proba(X_test)[:, 1] 

# 실제값 데이터 셋과 레이블 값이 1일 때의 예측 확률을 precision_recall_curve 인자로 입력 
precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1 )
print('반환된 분류 결정 임곗값 배열의 Shape:', thresholds.shape)
print('반환된 precisions 배열의 Shape:', precisions.shape)
print('반환된 recalls 배열의 Shape:', recalls.shape)
print('-'*50)

print("thresholds 5 sample:", thresholds[:5])
print("precisions 5 sample:", precisions[:5])
print("recalls 5 sample:", recalls[:5])
print('-'*50)

#반환된 임계값 배열 로우가 143건이므로 샘플로 10건만 추출하되, 임곗값을 15 Step으로 추출. 
thr_index = np.arange(0, thresholds.shape[0], 15)
print('샘플 추출을 위한 임계값 배열의 index 10개:', thr_index)
print('샘플용 10개의 임곗값: ', np.round(thresholds[thr_index], 2))
print('-'*50)

# 15 step 단위로 추출된 임계값에 따른 정밀도와 재현율 값 
print('샘플 임계값별 정밀도: ', np.round(precisions[thr_index], 3))
print('샘플 임계값별 재현율: ', np.round(recalls[thr_index], 3))
```

    반환된 분류 결정 임곗값 배열의 Shape: (143,)
    반환된 precisions 배열의 Shape: (144,)
    반환된 recalls 배열의 Shape: (144,)
    --------------------------------------------------
    thresholds 5 sample: [0.1038794  0.10388165 0.1039068  0.10782079 0.10885047]
    precisions 5 sample: [0.38853503 0.38461538 0.38709677 0.38961039 0.38562092]
    recalls 5 sample: [1.         0.98360656 0.98360656 0.98360656 0.96721311]
    --------------------------------------------------
    샘플 추출을 위한 임계값 배열의 index 10개: [  0  15  30  45  60  75  90 105 120 135]
    샘플용 10개의 임곗값:  [0.1  0.12 0.14 0.19 0.28 0.4  0.56 0.67 0.82 0.95]
    --------------------------------------------------
    샘플 임계값별 정밀도:  [0.389 0.44  0.466 0.539 0.647 0.729 0.836 0.949 0.958 1.   ]
    샘플 임계값별 재현율:  [1.    0.967 0.902 0.902 0.902 0.836 0.754 0.607 0.377 0.148]

<br>

```python
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def precision_recall_curve_plot(y_test , pred_proba_c1):
    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. 
    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)
    
    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시
    plt.figure(figsize=(8,6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')
    
    # threshold 값 X 축의 Scale을 0.1 단위로 변경
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1),2))
    
    # x축, y축 label과 legend, 그리고 grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend(); plt.grid()
    plt.show()
    
precision_recall_curve_plot( y_test, lr_clf.predict_proba(X_test)[:, 1] )
```

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/Accuracy_files/Accuracy.png)

<br>

## F1 스코어

- 정밀도와 재현율을 결합한 지표로, 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 상대적으로 높은 값을 가진다.
- `F1 스코어  = 2/((1/재현율) + (1/정밀도)) = 2*(재현율*정밀도)/(재현율+정밀도)`

- 사이킷런은 `f1_score()`를 제공한다

```python
from sklearn.metrics import f1_score 
f1 = f1_score(y_test , pred)
print('F1 스코어: {0:.4f}'.format(f1))
```

```
F1 스코어: 0.7805
```

<br>

```python
def get_clf_eval(y_test , pred):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    # F1 스코어 추가
    f1 = f1_score(y_test,pred)
    print('오차 행렬')
    print(confusion)
    # f1 score print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1:{3:.4f}'.format(accuracy, precision, recall, f1))

thresholds = [0.4 , 0.45 , 0.50 , 0.55 , 0.60]
pred_proba = lr_clf.predict_proba(X_test)
get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)
```

```
임계값: 0.4
오차 행렬
[[98 20]
 [10 51]]
정확도: 0.8324, 정밀도: 0.7183, 재현율: 0.8361, F1:0.7727
--------------------------------------------------
임계값: 0.45
오차 행렬
[[103  15]
 [ 12  49]]
정확도: 0.8492, 정밀도: 0.7656, 재현율: 0.8033, F1:0.7840
--------------------------------------------------
임계값: 0.5
오차 행렬
[[104  14]
 [ 13  48]]
정확도: 0.8492, 정밀도: 0.7742, 재현율: 0.7869, F1:0.7805
--------------------------------------------------
임계값: 0.55
오차 행렬
[[109   9]
 [ 15  46]]
정확도: 0.8659, 정밀도: 0.8364, 재현율: 0.7541, F1:0.7931
--------------------------------------------------
임계값: 0.6
오차 행렬
[[112   6]
 [ 16  45]]
정확도: 0.8771, 정밀도: 0.8824, 재현율: 0.7377, F1:0.8036
--------------------------------------------------
```

<br>

## ROC곡선과 AUC(Area Under Curve, ROC 곡선의 면적)

- FPR이 변할 때, TPR이 어떻게 변하는지 나ㅏ내는 곡선
- x축을 FPR(False Positive Rate) : `FP/(FP + TN)` 실제 음성(Negative)을 잘못예측한 비율. 1 - TNR = 1 - 특이성
- y축을 TPR(True Positive Rate,재현율,민감도) : `TP/(FN + TP)`

- AUC : ROC 곡선 밑의 면적을 구한 것으로, 1에 가까울수록 가장 좋은 수치
- 사이킷런은 `roc_curve()`, `roc_auc_score()`를 제공한다

<br>

---

Reference

파이썬 머신러닝 완벽 가이드

