---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day31) Logistic Regression 
tags:
  - 머신러닝
---

<br>

### Sigmoid함수 : 2진분류의 활성화 함수

- 이진 분류는 0 또는 1로 나뉜다. (ex. spam(1) or ham(0))

- Linear Regression의 예측함수 H(x)=Wx+b는 예측 결과 값이 1보다 훨씬 크거나 0보다 훨씬 작을 수 있다.
- 그래서 0~1 사이의 값을 가지는 sigmoid함수를 사용

```python
import math
import matplotlib.pyplot as plt

#Logistic Hypothesis
def sigmoid(z):
    return 1./(1.+math.e**-z)

print(sigmoid(0))
```

    0.5

```python
#시각화
x,y = [],[]
for k in range(-100,101):
    n = sigmoid(k/10)
    x.append(k/10)
    y.append(n)
    
plt.plot(x,y,'r')
plt.grid()
plt.show()
```


![png](/Users/dahyejung/Desktop/Python/tensorflow/sigmoid_files/sigmoid_3_0.png)

<br>

### Logistic Regression의 cost 함수 설명


```python
import tensorflow as tf
import numpy as np
tf.random.set_seed(5)
```


```python
# x_data : [6,2]
x_data = [[1,2],
          [2,3],
          [3,1],
          [4,3],
          [5,3],
          [6,2]]

# y_data : [6,1]
y_data = [[0],
          [0],
          [0],
          [1],
          [1],
          [1]]

x_train = np.array(x_data,dtype=np.float32)
y_train = np.array(y_data,dtype=np.float32)
```


```python
#변수초기화
#(6,2)*(2,1) => (6*1)
W = tf.Variable(tf.random.normal([2,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')
W
```




    <tf.Variable 'weight:0' shape=(2, 1) dtype=float32, numpy=
    array([[-0.18030666],
           [-0.95028627]], dtype=float32)>




```python
#hypothesis : 예측함수, H(x) = sigmoid(X*W+b)
def hypothesis(X):
    return sigmoid(tf.matmul(X,W)+b) #0과1사이의 값을 출력
```


```python
#비용함수, logloss
def cost_func():
    #cost = tf.reduce_mean(-y_train*tf.math.log(hypothesis(x_train))-(1-y_train)*tf.math.log(1-hypothesis(x_train)))
    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + (1-y_train)*tf.math.log(1-hypothesis(x_train)))
    return cost

optimizer = tf.keras.optimizers.Adam(lr=0.01)
```


```python
#학습시작
print("start learnig")
for step in range(10001):
    #cost를 minimize
    optimizer.minimize(cost_func,var_list = [W,b])
    if step%1000 == 0:
        print('%04d'%step,'cost: ',cost_func().numpy(),'W:',W.numpy(),'b:',b.numpy())
        print()
print("learning finish")
```

    start learnig
    0000 cost:  1.661137 W: [[-0.17030667]
     [-0.9402863 ]] b: [0.23652855]
    
    1000 cost:  0.17934637 W: [[1.3178085 ]
     [0.19723557]] b: [-4.7614303]
    
    2000 cost:  0.07253331 W: [[2.0121026]
     [0.8405848]] b: [-8.711954]
    
    3000 cost:  0.037006143 W: [[2.6189137]
     [1.2299302]] b: [-11.698347]
    
    4000 cost:  0.020763235 W: [[3.1709244]
     [1.5396552]] b: [-14.285828]
    
    5000 cost:  0.012184973 W: [[3.6927986]
     [1.8153361]] b: [-16.680555]
    
    6000 cost:  0.0073189805 W: [[4.197647 ]
     [2.0743663]] b: [-18.973686]
    
    7000 cost:  0.0044522597 W: [[4.6926694]
     [2.3246486]] b: [-21.210659]
    
    8000 cost:  0.0027277553 W: [[5.1819415]
     [2.5701625]] b: [-23.415737]
    
    9000 cost:  0.0016780436 W: [[5.667812 ]
     [2.8129678]] b: [-25.602293]
    
    10000 cost:  0.001034755 W: [[6.151635 ]
     [3.0542402]] b: [-27.777964]
    
    learning finish



```python
#회귀계수 출력
print('Weight',W.numpy())
print('bias',b.numpy())
```

    Weight [[6.151635 ]
     [3.0542402]]
    bias [-27.777964]



```python
# 정확도 측정 : accuracy computation
#hypothesis(x_train).numpy()
def predict(X):
    return tf.cast(hypothesis(X)>0.5,dtype=tf.float32)   #타입변환 함수

preds = predict(x_train)
tf.cast(tf.equal(preds,y_train),dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y_train),dtype=tf.float32))
print('accuracy',accuracy.numpy())
print('preds',preds.numpy())
print('hypothesis',hypothesis(x_train).numpy())
```

    accuracy 1.0
    preds [[0.]
     [0.]
     [0.]
     [1.]
     [1.]
     [1.]]
    hypothesis [[1.8225705e-07]
     [1.8111663e-03]
     [1.8908884e-03]
     [9.9750584e-01]
     [9.9999464e-01]
     [9.9999976e-01]]

