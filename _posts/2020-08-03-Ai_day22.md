---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day22,23) 분류(Classfication)와 결정트리(Decision Tree) 
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

## 분류(Classification) 

> 학습 데이터로 주어진 데이터의 피쳐와 레이블값(결정값, 클래스값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을  때 미지의 레이블 값을 예측

<br>

### sklearn의 분류 모델 클래스

- Decision Tree
- Logistic Regression
- Naive Bayes(나이브 베이즈)
- Support Vector Machine
- Nearest Neighbor(최소근접 알고리즘)

<br>

## 결정트리 (Decision Tree)

- 대표적인 분류 학습 모델, 회귀도 가능, 스무고개와 유사
- 루트 노드 --> 규칙 노드(중간노드) --> 리프노드(끝노드)
- 장점 : 쉽고 직관적이다. 피쳐 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않다.
- 단점 : 과적합으로 알고리즘 성능이 떨어진다. 따라서 트리의 크기를 사전에 제한하는 튜닝이 필요

<br>

### 용어 정리

- <b>불순도(Impurity)</b>

  해당범주안에 서로 다른 데이터가 얼마나 섞여 있는지를 말한다, 불확실성, 무질서도<br>

- <b>데이터의 균일도</b>

  다양성이 낮을수록 데이터의 균일도는 높다. <br>
  예를 들어 검은공과 하얀공이 적절히 섞인 A, 약간의 하얀공과 대부분의 검은공으로 이루어져 있는 B, 모두 검은공으로 이루어져 있는 C가 있다고 할 때, 
  C > B > A 순으로 균일도가 높다고 할 수 있다. 

- <b>균일도 기반 규칙조건</b>

  정보 균일도가 높은 데이터세트를 먼저 선택하도록 규칙조건을 만들어야 한다.

- <b>정보 균일도 측정방법</b>

  대표적인 방법으로는 엔트로피를 이용한 정보이득 지수와 지니 계수가 있다.

- <b>엔트로피</b>

  주어진 데이터 집합의 혼잡도를 의미한다. 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮다.

- <b>정보이득(Information Gain)</b>

  정보이득 = 1 - 엔트로피지수
  즉, 데이터의 혼잡도가 작을수록(엔트로피가 낮을수록) 정보이득 지수는 커진다. 결정트리는 정보이득이 높은 속성을 기준으로 분할한다.

- <b>지니계수</b>

  1 - (각 범주별 데이터 비율의 제곱의 합), 0 이면 최소(끝노드) , 1이면 최대¶<br>
  지니 불순도는 집합에 이질적인 것이 얼마나 섞였는지를 측정하는 지표이다. 어떤 집합에서 한 항목을 뽑아 무작위로 라벨을 추정할 때 틀릴 확률을 말한다. 집합에 있는 항목이 모두 같다면 지니 불순도는 최솟값(0)을 갖게 되며 이 집합은 완전히 순수하다고 할 수 있다.

<br>

###  Iris(붓꽃) 품종 예측하기


```python
from sklearn.datasets import load_iris 
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```


```python
iris = load_iris()
iris #sklearn.utils.Bunch

#x값, 피쳐만 추출
iris_data = iris.data
#print(iris_data) #(150, 4) : 2차원 ndarray
print(iris.feature_names) #sepal:꽃받침, petal:꽃잎
```

    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

<br>

```python
#y값, 답(label)만 추출
iris_label = iris.target
print(iris_label)
print(iris.target_names)  #iris의 품종 : [0:'setosa', 1:'versicolor', 2:'virginica']
```

    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
     2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
     2 2]
    ['setosa' 'versicolor' 'virginica']

<br>

```python
iris_df = pd.DataFrame(data = iris_data, columns=iris.feature_names)
iris_df['label'] = iris_label
print(iris_df['label'].value_counts())
iris_df.head()
```

    2    50
    1    50
    0    50
    Name: label, dtype: int64





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

</div>

<br>

### train data set과 test데이터로 분리하기

- scikit-learn.model_selection 패키지 안에 train_test_split 모듈을 활용하여 손쉽게 train set과 test set을 분리할 수 있다.
- 옵션값
  - `test_size` : 테스트 셋 구성의 비율을 나타냅니다. 0.2는 전체 데이터 셋의 20%를 테스트 셋으로 지정하겠다는 의미. default 값은 0.25
  - `shuffle` : default=True. split을 해주기 이전에 섞을건지 
  - `stratify` : default=None 입니다. stratify 값을 label로 지정해주면 각각의 class 비율을 train data set과 test data set에 유지해 줍니다. (어느 한쪽에 쏠려서 분배되는 것을 방지하기 위해)
  - `random_state` : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)


```python
X_train,X_test,y_train,y_test = train_test_split(iris_data,iris_label,test_size=0.2,random_state=11, stratify = iris_label)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (120, 4) (30, 4) (120,) (30,)

<br>


```python
dt_clf = DecisionTreeClassifier(random_state=11)
dt_clf.fit(X_train,y_train) #DecisionTreeClassifier 학습
```


    DecisionTreeClassifier(random_state=11)

<br>

### 결정트리의 주요 하이퍼 파라미터

- `max_depth` : 트리의 최대 깊이를 규정
- `max_features` : 최적의 분할을 위해 고려해야 할 최대 피쳐 갯수
- `min_sample_split` : 샘플이 최소한 몇 개 이상이어야 split(하위노드로 분리), 과적합 제어하기 위해 사용 (default=2)
- `min_sample_leaf` : 말단노드가 되기위한 최소한의 샘플 데이터 수, 노드가 되려면 가지고 있어야할 최소 샘플 수
- `max_leaf_nodes` : 말단 노드의 최대 갯수

```python
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=11, splitter='best')
```

<br>


```python
pred = dt_clf.predict(X_test)
pred,y_test
```


    (array([2, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,
            0, 1, 0, 0, 2, 1, 0, 1]),
     array([2, 2, 2, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,
            0, 1, 0, 0, 2, 1, 0, 1]))

<br>


```python
#정확도 측정 : accuracy
from sklearn.metrics import accuracy_score,classification_report
print('정확도:{:.4f}'.format(accuracy_score(y_test,pred)))
```

    정확도:0.9333

<br>

### 결정트리 모델의 시각화


```python
from sklearn.tree import export_graphviz
export_graphviz(dt_clf,out_file='tree.dot', class_names=iris.target_names, feature_names = iris.feature_names, impurity=True, filled=True)
```

```shell
### graphviz 설치방법
brew install graphviz
pip install pyparsing
pip install graphviz
pip install pydot
conda install graphviz
```

<br>


```python
import graphviz

f = open('tree.dot')
dot_graph = f.read()
graphviz.Source(dot_graph)
```

<br>

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/graphviz_files/tree.svg)

<br>

### 내부 알고리즘 처리과정 분석


```python
#feature importance : 결정트리의 규칙노드가 분기 조건으로 사용할 피쳐의 우선순위
print('feature importnace:',dt_clf.feature_importances_)
print()
for name,value in zip(iris.feature_names,dt_clf.feature_importances_):
    print('{}:{:.3f}'.format(name,value))
    
import seaborn as sns
sns.barplot(x=dt_clf.feature_importances_,y=iris.feature_names)
```

    feature importnace: [0.02500521 0.         0.04867657 0.92631822]
    
    sepal length (cm):0.025
    sepal width (cm):0.000
    petal length (cm):0.049
    petal width (cm):0.926





    <matplotlib.axes._subplots.AxesSubplot at 0x1a1873d6d0>

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/Classfication_TreeDecision_files/barplot.png)

<br>

```python
# 지니계수 : 1 - (각 범주별 데이터 비율의 제곱의 합)
def gini(values):
    sum_of_list = sum(values)
    squared_sum =0
    for value in values:
        squared_sum += (value/sum_of_list)**2
    return round(1-squared_sum,3)
```


```python
first_df = pd.DataFrame(data = X_train, columns = iris.feature_names)
first_df['label'] = y_train
#first_df #(120, 5)
print(first_df['label'].value_counts())
```

    0    41
    1    40
    2    39
    Name: label, dtype: int64

<br>

```python
# root node 
# 첫번째 규칙 : petal width <= 0.8

#feature importance가 가장 높은 피쳐인 'petal width (cm)'를 규칙으로 사용
value = [41,40,39]
print(gini(value))
print()
print(first_df[first_df['label']==0]['petal width (cm)'].min())
print(first_df[first_df['label']==0]['petal width (cm)'].max())
print(value[0])
print()
print(first_df[first_df['label']==1]['petal width (cm)'].min())
print(first_df[first_df['label']==1]['petal width (cm)'].max())
print(value[1])
print()
print(first_df[first_df['label']==2]['petal width (cm)'].min())
print(first_df[first_df['label']==2]['petal width (cm)'].max())
print(value[2])
print()
```

    0.667
    
    0.1
    0.6
    41
    
    1.0
    1.8
    40
    
    1.4
    2.5
    39

<br>


```python
#자식 노드 생성
second_node = first_df[first_df['petal width (cm)']<=0.8]  
third_node = first_df[first_df['petal width (cm)']>0.8]  
```


```python
#두번째 노드
print(second_node['label'].value_counts())
print()

value = [41,0,0]
print(gini(value)) #지니계수가 0 => leaf node

setosa_result = second_node.copy()
```

    0    41
    Name: label, dtype: int64
    
    0.0

<br>

```python
#세번째 노드
print(third_node['label'].value_counts())
print()

value = [0,40,39]
print(gini(value)) 
```

    1    40
    2    39
    Name: label, dtype: int64
    
    0.5

<br>

```python
#자식 노드 생성
fourth_node = third_node[third_node['petal width (cm)']<=1.55]  
fifth_node = third_node[third_node['petal width (cm)']>1.55]  
```


```python
#네번째 노드
print(fourth_node['label'].value_counts())
print()

value = [0,37,1]   #train데이터에서는 이 상태에서 다시 split해서 더 정확하게 분류할 수 있지만, 
                   #이런식으로 깊이가 계속 깊어지면 test데이터를 넣었을때 과적합(overfitting) 발생할 확률이 커짐
print(gini(value)) 
print()

#feature imortance가 두번째로 높은 'petal length'를 피쳐로 선택
```

    1    37
    2     1
    Name: label, dtype: int64
    
    0.051

<br>

### 결정 트리의 하이퍼 파라미터 튜닝 : 학습시간 및 과적합 제어

### 과적합

- 기계 학습(machine learning)에서 학습 데이타를 과하게 학습(overfitting)하는 것을 뜻한다. 일반적으로 학습 데이타는 실제 데이타의 부분 집합이므로 학습데이타에 대해서는 오차가 감소하지만 test 데이타에 대해서는 오차가 증가하게 된다.
- 피쳐가 지나치게 많거나, 트리가 지나치게 깊을 때 발생

<br>

### `max_depth` 값 조정 : 트리의 최대 깊이를 규정


```python
#dt_clf = DecisionTreeClassifier(random_state=11) #0.9333
#dt_clf = DecisionTreeClassifier(random_state=11,max_depth=1) #0.6333
#dt_clf = DecisionTreeClassifier(random_state=11,max_depth=2) #0.8667
dt_clf = DecisionTreeClassifier(random_state=11,max_depth=3) #0.9333
#dt_clf = DecisionTreeClassifier(random_state=11,max_depth=4) #0.9333

dt_clf.fit(X_train,y_train)

pred = dt_clf.predict(X_test)
print(pred,y_test)

#from sklearn.metrics import accuracy_score
print('정확도 {:.4f}'.format(accuracy_score(pred,y_test)))
```

    [2 2 1 1 2 0 1 0 0 1 1 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1] [2 2 2 1 2 0 1 0 0 1 2 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1]
    정확도 0.9333

<br>

### `min_samples_split` 값 조정 : 샘플이 최소한 몇 개 이상이어야 split(하위노드로 분리)할지 결정. 샘플 갯수가 이 값보다 작으면 split 하지 않는다.


```python
#dt_clf = DecisionTreeClassifier(random_state=11,min_samples_split=4) #0.9333
dt_clf = DecisionTreeClassifier(random_state=11,min_samples_split=41) #0.9333

dt_clf.fit(X_train,y_train)

pred = dt_clf.predict(X_test)
print(pred,y_test)

print('정확도 {:.4f}'.format(accuracy_score(pred,y_test)))
```

    [2 2 1 1 2 0 1 0 0 1 1 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1] [2 2 2 1 2 0 1 0 0 1 2 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1]
    정확도 0.9333

<br>

### `min_samples_leaf` 값 조정 : 말단노드가 되기 위한 최소한의 샘플 데이터 수


```python
#dt_clf = DecisionTreeClassifier(random_state=11,min_samples_leaf=4) #0.9333
dt_clf = DecisionTreeClassifier(random_state=11,min_samples_leaf=39) #0.9333

dt_clf.fit(X_train,y_train)

pred = dt_clf.predict(X_test)
print(pred,y_test)

print('정확도 {:.4f}'.format(accuracy_score(pred,y_test)))
```

    [2 2 1 1 2 0 1 0 0 1 1 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1] [2 2 2 1 2 0 1 0 0 1 2 1 1 2 2 0 2 1 2 2 1 0 0 1 0 0 2 1 0 1]
    정확도 0.9333

<br>

```python
from sklearn.tree import export_graphviz
export_graphviz(dt_clf,out_file='tree.dot', class_names=iris.target_names, feature_names = iris.feature_names, impurity=True, filled=True)

import graphviz

f = open('tree.dot')
dot_graph = f.read()
graphviz.Source(dot_graph)
```

![png](https://raw.githubusercontent.com/zoe0-0/blog/master/images/Classfication_TreeDecision_files/graphviz.svg)



