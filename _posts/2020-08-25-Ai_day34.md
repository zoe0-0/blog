---
layout: post
title: (AI이노베이션스퀘어_기본과정_Day33,34) XOR문제, Neural Net으로 해결하기
tags:
  - AI이노베이션스퀘어_기본과정
---

<br>

>  [모두를 위한 머신러닝과 딥러닝의 강의(김성훈교수님)](https://hunkim.github.io/ml/)를 참조해서 정리한 수업내용 입니다.

<br>

### [퍼셉트론](https://zoe0-0.github.io/blog/Ai_day18/)

> 위 포스팅에서도 확인할 수 있듯이, 단층 퍼셉트론으로는 XOR 문제를 해결할 수 없다.

<br>

### [인공신경망(Artificial Neural Network, ANN)](http://tcpschool.com/deep2018/deep2018_deeplearning_intro)

> 딥러닝에서 가장 기본이 되는 개념은 바로 신경망(Neural Network)입니다.
>
> 신경망이란 인간의 뇌가 가지는 생물학적 특성 중 뉴런의 연결 구조를 가리키며, 이러한 신경망을 본떠 만든 네트워크 구조를 인공신경망(Artificial Neural Network, ANN)이라고 부릅니다. (인공지능 분야에서 신경망이란 보통 인공신경망을 지칭하며, 따라서 인공신경망을 따로 구분하지 않고 신경망이라고 부르기도 합니다.)
>
> 인간의 뇌에는 약 1,000억 개의 수많은 뉴런 즉 신경세포가 존재하며, 하나의 뉴런은 다른 뉴런에게서 신호를 받고 또 다른 뉴런에게 신호를 전달하는 단순한 역할만을 수행합니다. 하지만 인간의 뇌는 이러한 수많은 뉴런이 모여 만든 신호의 흐름을 기반으로 다양한 사고를 할 수 있게 되며, 이것을 컴퓨터로 구현하도록 노력한 것이 바로 인공신경망입니다.
>
> 인공신경망은 여러 뉴런이 서로 연결되어 있는 구조의 네트워크이며, 입력층(input layer)를 통해 학습하고자 하는 데이터를 입력받게 됩니다. 이렇게 입력된 데이터들은 여러 단계의 은닉층(hidden layer)을 지나면서 처리가 이루어져 출력층(output layer)을 통해 최종 결과가 출력되게 됩니다.
>
> 이러한 신경망을 3개 이상 중첩한 구조를 깊은 신경망(Deep Neural Network, DNN)이라고 부르며, 이를 활용한 머신러닝 학습을 특별히 딥러닝이라고 부르는 것입니다.
>
> 일반적으로 인공신경망이란 다층 퍼셉트론의 조합이라 할 수 있습니다

<br>

## XOR using NN

---

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/1.png" alt="png" style="zoom:40%;" />

3개의 퍼셉트론을 통해 XOR 문제를 풀 수 있다.

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/2.png" alt="png" style="zoom:40%;" />

왼쪽 두개의 퍼셉트론에서 각각의 W와 b를 Multinomial Classfication 처럼 하나의 Vector로 만들 수 있다. 

<br>

<img src="https://raw.githubusercontent.com/zoe0-0/blog/master/images/nn/3.png" alt="png" style="zoom:40%;" />

수식으로 표현하면 위 그림과 같다

<br>

## 텐서플로우로 구현해보기

---

```python
#단층 신경망으로 해결할 수 없던 XOR문제를 다층 신경망으로 해결함

import tensorflow as tf
import numpy as np
tf.random.set_seed(5)

# train data set 
x_data = [[0,0],
          [0,1],
          [1,0],
          [1,1]]

y_data = [[0],
          [1],
          [1],
          [0]]

x_train = np.array(x_data,dtype=np.float32)
y_train = np.array(y_data,dtype=np.float32)
```


```python
# Layer1
W1 = tf.Variable(tf.random.normal([2,2]),name = 'weight')
b1 = tf.Variable(tf.random.normal([2]), name = 'bias')

def layer1(X):
    return tf.sigmoid(tf.matmul(X,W1) + b1 )
```


```python
# (4,2) * (2,1) = (4,1)
W2 = tf.Variable(tf.random.normal([2,1]),name = 'weight')
b2 = tf.Variable(tf.random.normal([1]), name = 'bias')

# hypothesis 예측 함수(방정식)  , H(x) = sigmoid(W * X + b)
def hypothesis(X):
    return  tf.sigmoid(tf.matmul(layer1(X),W2) + b2 )
```


```python
# 비용함수 : logloss
def cost_func():
    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + (1-y_train)*
                       tf.math.log(1-hypothesis(x_train)))
    return cost
```


```python
# 경사 하강법
# learning rate (학습율) 을 0.01로 설정하여  optimizer객체를 생성
# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
optimizer = tf.keras.optimizers.Adam(lr=0.01)
```


```python
# 학습 시작
print('***** Start Learning!!')
for step in range(5001):
    # cost를 minimize한다
    optimizer.minimize(cost_func, var_list=[W1,b1,W2,b2])
    
    if step % 1000 == 0:
        print('%04d'%step, 'cost: [', cost_func().numpy(), ']  W1: ', W1.numpy(), '  b1: ', b1.numpy(),'W2: ', W2.numpy(), '  b2: ', b2.numpy())
        
print('***** Learning Finished!!')
```

    ***** Start Learning!!
    0000 cost: [ 0.7040971 ]  W1:  [[-0.19030488 -0.9402918 ]
     [-0.04963873 -0.73254627]]   b1:  [0.21652977 0.8206513 ] W2:  [[ 1.4565471]
     [-1.0348129]]   b2:  [0.02700325]
    1000 cost: [ 0.02550175 ]  W1:  [[-5.024761  -6.6669364]
     [-4.994727  -6.5032763]]   b1:  [7.5096507 2.915475 ] W2:  [[ 8.87536 ]
     [-8.637342]]   b2:  [-4.2820168]
    2000 cost: [ 0.0068961554 ]  W1:  [[-5.944842 -7.574677]
     [-5.917539 -7.41493 ]]   b1:  [8.922688 3.408215] W2:  [[ 11.296161]
     [-10.965301]]   b2:  [-5.5542603]
    3000 cost: [ 0.0029871406 ]  W1:  [[-6.4519587 -8.081456 ]
     [-6.426093  -7.9230824]]   b1:  [9.694395 3.674915] W2:  [[ 12.855274]
     [-12.503557]]   b2:  [-6.3560357]
    4000 cost: [ 0.0015255288 ]  W1:  [[-6.8228345 -8.453764 ]
     [-6.797982  -8.296077 ]]   b1:  [10.256575   3.8682897] W2:  [[ 14.117276]
     [-13.758825]]   b2:  [-6.999548]
    5000 cost: [ 0.00084185496 ]  W1:  [[-7.127812  -8.760641 ]
     [-7.1037564 -8.603351 ]]   b1:  [10.717856   4.0264554] W2:  [[ 15.241033]
     [-14.8807  ]]   b2:  [-7.569965]
    ***** Learning Finished!!

```python
# predict 
# accuracy computation (정확도 측정)
def predict(X):
    return tf.cast(hypothesis(X) > 0.5, dtype = tf.float32)

accuracy = tf.reduce_mean(tf.cast(tf.equal(predict(x_train),y_train),
                                     dtype = tf.float32))

print("Hypothesis:\n",hypothesis(x_train).numpy(), 
      "\nPredict:\n",predict(x_train).numpy(),
      "\nAccuracy:",accuracy.numpy())   # Accuracy: 0.5
```

    Hypothesis:
     [[9.5849548e-04]
     [9.9919170e-01]
     [9.9920160e-01]
     [8.0075976e-04]] 
    Predict:
     [[0.]
     [1.]
     [1.]
     [0.]] 
    Accuracy: 1.0











